{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index llama-index-llms-huggingface\n",
        "!pip install llama-index-llms-google-genai llama-index"
      ],
      "metadata": {
        "id": "oW1g3Bx0FWaf"
      },
      "id": "oW1g3Bx0FWaf",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install youtube-transcript-api # not working\n",
        "#!pip install -U yt-dlp # not working\n",
        "!pip install -U openai-whisper\n",
        "!pip install llama-index-tools-wikipedia llama-index-tools-tavily-research"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ckmnFnks6EFE",
        "outputId": "e5f14cbd-e7da-4a9c-d38c-9b06f63e776c"
      },
      "id": "ckmnFnks6EFE",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-1.2.2-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (2025.7.14)\n",
            "Downloading youtube_transcript_api-1.2.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.0/485.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-1.2.2\n",
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=f5e95a573e192dbbff7f8b51c632061d908bfc630ca9db876a5e205baa5cdc6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/d2/9a/801b5cc5b2a1af2e280089b71c326711a682fc1d50ea29d0ed\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n",
            "Collecting llama-index-tools-wikipedia\n",
            "  Downloading llama_index_tools_wikipedia-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-tools-tavily-research\n",
            "  Downloading llama_index_tools_tavily_research-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: llama-index-core<0.14,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-tools-wikipedia) (0.13.0)\n",
            "Collecting wikipedia<2.0,>=1.4 (from llama-index-tools-wikipedia)\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tavily-python>=0.2.4 (from llama-index-tools-tavily-research)\n",
            "  Downloading tavily_python-0.7.10-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (3.12.14)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (1.2.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (4.3.8)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (4.14.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (1.17.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia<2.0,>=1.4->llama-index-tools-wikipedia) (4.13.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (1.20.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (1.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (3.1.6)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (1.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia<2.0,>=1.4->llama-index-tools-wikipedia) (2.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (3.26.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (0.16.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (25.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (1.3.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-tools-wikipedia) (3.0.2)\n",
            "Downloading llama_index_tools_wikipedia-0.4.0-py3-none-any.whl (3.5 kB)\n",
            "Downloading llama_index_tools_tavily_research-0.4.0-py3-none-any.whl (4.2 kB)\n",
            "Downloading tavily_python-0.7.10-py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11757 sha256=6a4ae18ab365fd4d6c3061160289bd1317e1a49e974e04d1384a95e6bb520516\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia, tavily-python, llama-index-tools-wikipedia, llama-index-tools-tavily-research\n",
            "Successfully installed llama-index-tools-tavily-research-0.4.0 llama-index-tools-wikipedia-0.4.0 tavily-python-0.7.10 wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa04d456-f7b5-4f59-a4d4-eea0ff47cfc3",
      "metadata": {
        "id": "fa04d456-f7b5-4f59-a4d4-eea0ff47cfc3"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3d17d217-f6da-48c8-80dc-acc94213a50d",
      "metadata": {
        "id": "3d17d217-f6da-48c8-80dc-acc94213a50d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a71aa808-8d78-497b-a43e-cb65ded30565",
      "metadata": {
        "id": "a71aa808-8d78-497b-a43e-cb65ded30565"
      },
      "source": [
        "# Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f56343d7-877a-4c78-9cd4-0d0a33be7838",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f56343d7-877a-4c78-9cd4-0d0a33be7838",
        "outputId": "851dbeaa-8aae-4935-cab4-d4589e744059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "{'task_id': '8e867cd7-cff9-4e6c-867a-ff5ddc2550be', 'question': 'How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.', 'Level': '1', 'file_name': ''}\n"
          ]
        }
      ],
      "source": [
        "#Before running this cell make sure to add \"questions.json\" in files\n",
        "\n",
        "# Load the JSON file\n",
        "with open(\"questions.json\", \"r\", encoding=\"utf-8-sig\") as f:\n",
        "    questions_data = json.load(f)\n",
        "\n",
        "print(type(questions_data))\n",
        "print(questions_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a4d0b9d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4d0b9d9",
        "outputId": "47d2be10-64ea-405d-87d9-2f884345d380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'task_id': '99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3', 'question': 'Hi, I\\'m making a pie but I could use some help with my shopping list. I have everything I need for the crust, but I\\'m not sure about the filling. I got the recipe from my friend Aditi, but she left it as a voice memo and the speaker on my phone is buzzing so I can\\'t quite make out what she\\'s saying. Could you please listen to the recipe and list all of the ingredients that my friend described? I only want the ingredients for the filling, as I have everything I need to make my favorite pie crust. I\\'ve attached the recipe as Strawberry pie.mp3.\\n\\nIn your response, please only list the ingredients, not any measurements. So if the recipe calls for \"a pinch of salt\" or \"two cups of ripe strawberries\" the ingredients on the list would be \"salt\" and \"ripe strawberries\".\\n\\nPlease format your response as a comma separated list of ingredients. Also, please alphabetize the ingredients.', 'Level': '1', 'file_name': '99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3.mp3'}\n"
          ]
        }
      ],
      "source": [
        "# Select random question\n",
        "item  = random.choice(questions_data)\n",
        "item = {'task_id': '99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3', 'question': 'Hi, I\\'m making a pie but I could use some help with my shopping list. I have everything I need for the crust, but I\\'m not sure about the filling. I got the recipe from my friend Aditi, but she left it as a voice memo and the speaker on my phone is buzzing so I can\\'t quite make out what she\\'s saying. Could you please listen to the recipe and list all of the ingredients that my friend described? I only want the ingredients for the filling, as I have everything I need to make my favorite pie crust. I\\'ve attached the recipe as Strawberry pie.mp3.\\n\\nIn your response, please only list the ingredients, not any measurements. So if the recipe calls for \"a pinch of salt\" or \"two cups of ripe strawberries\" the ingredients on the list would be \"salt\" and \"ripe strawberries\".\\n\\nPlease format your response as a comma separated list of ingredients. Also, please alphabetize the ingredients.', 'Level': '1', 'file_name': '99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3.mp3'}\n",
        "\n",
        "print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d65e0128",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d65e0128",
        "outputId": "7e21624a-0be1-4f13-84f1-213cee5e07d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3.mp3\n"
          ]
        }
      ],
      "source": [
        "question_text = item.get('question')\n",
        "file_name = item.get(\"file_name\")\n",
        "file_name = file_name if file_name != \"\" else None\n",
        "task_id = item.get(\"task_id\")\n",
        "\n",
        "print(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3034234",
      "metadata": {
        "id": "f3034234"
      },
      "source": [
        "# Set up LLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "mvbbz05-JQG0"
      },
      "id": "mvbbz05-JQG0",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "\n",
        "llm = GoogleGenAI(\n",
        "        model=\"models/gemini-2.0-flash-lite\",\n",
        "        api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "ldwSoHBEcy4E"
      },
      "id": "ldwSoHBEcy4E",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#llm.complete('what is the capital of italy?')"
      ],
      "metadata": {
        "id": "aM5EyRxesYUZ"
      },
      "id": "aM5EyRxesYUZ",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b645628e",
      "metadata": {
        "id": "b645628e"
      },
      "source": [
        "# Set up Llamaindex (tools, agents, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "eac5f4ab",
      "metadata": {
        "id": "eac5f4ab"
      },
      "outputs": [],
      "source": [
        "# calculator tool\n",
        "\n",
        "def calculate(input: dict) -> dict:\n",
        "    \"\"\"Simple calculator function\"\"\"\n",
        "\n",
        "    expression = input['input']\n",
        "\n",
        "    # Remove any potentially unsafe operations\n",
        "    if any(unsafe in expression for unsafe in [\"import\", \"exec\", \"eval\", \"compile\", \"open\", \"__\"]):\n",
        "        return {\"error\": \"Unsafe expression\"}\n",
        "\n",
        "    try:\n",
        "        # Use a safer approach to evaluate mathematical expressions\n",
        "        # This is a simplified version - in production you'd want more safeguards\n",
        "        allowed_symbols = {\n",
        "            'sqrt': np.sqrt, 'pi': np.pi, 'e': np.e,\n",
        "            'sin': np.sin, 'cos': np.cos, 'tan': np.tan,\n",
        "            'log': np.log, 'log10': np.log10, 'exp': np.exp,\n",
        "            'floor': np.floor, 'ceil': np.ceil, 'abs': abs\n",
        "        }\n",
        "\n",
        "        # Replace common math operations with Python syntax\n",
        "        expression = expression.replace('^', '**')\n",
        "        result = eval(expression, {\"__builtins__\": {}}, allowed_symbols)\n",
        "        return {\"result\": str(result)}\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Failed to calculate: {str(e)}\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate({'input': 'sqrt(25)'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGg2qQ4RCIgf",
        "outputId": "79f7815c-11a7-48ec-f08d-e9fa01e9a7b7"
      },
      "id": "aGg2qQ4RCIgf",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'result': '5.0'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "5578a782",
      "metadata": {
        "id": "5578a782"
      },
      "outputs": [],
      "source": [
        "# youtube tool --> i cannot make it run for free as of now\n",
        "'''\n",
        "from llama_index.core import Document\n",
        "import subprocess\n",
        "\n",
        "def get_youtube_transcript(input: dict, cookies: str) -> dict:\n",
        "    \"\"\"Fetch transcript from YouTube based on video ID.\"\"\"\n",
        "\n",
        "    url = input['file_name']\n",
        "    video_id = url.split(\"v=\")[-1]\n",
        "\n",
        "    try:\n",
        "      # download audio\n",
        "      command = [\n",
        "          \"yt-dlp\",\n",
        "          \"-x\",\n",
        "          \"--audio-format\", \"mp3\",\n",
        "          \"-o\", f\"{video_id}.%(ext)s\",\n",
        "          url,\n",
        "          \"--cookies\", cookies,\n",
        "      ]\n",
        "\n",
        "      subprocess.run(command)\n",
        "    except:\n",
        "      return {'error': f\"Failed to download audio: {str(e)}\"}\n",
        "    return get_audio_transcript(f\"/content/{video_id}.mp3\")\n",
        "\n",
        "\n",
        "#from youtube_transcript_api import YouTubeTranscriptApi\n",
        "def get_youtube_transcript(input: dict, cookies: str) -> dict:\n",
        "    \"\"\"Fetch transcript from YouTube based on video ID.\"\"\"\n",
        "\n",
        "    url = input['file_name']\n",
        "    video_id = url.split(\"v=\")[-1]\n",
        "\n",
        "    try:\n",
        "        ytt_api = YouTubeTranscriptApi()\n",
        "        fetched_transcript = ytt_api.fetch(video_id)\n",
        "        text = \"\\n\".join([t[\"text\"] for t in fetched_transcript])\n",
        "        document = Document(text=text)\n",
        "        return {'result': document.text}\n",
        "    except Exception as e:\n",
        "        return {'error': f\"Failed to fetch transcript: {str(e)}\"}\n",
        "''';"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_youtube_transcript({'file_name': 'https://www.youtube.com/watch?v=L1vXCYZAYYM'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqGj2N4gUPz_",
        "outputId": "aeb08ff6-41cb-4c99-8674-eed4729b48f0"
      },
      "id": "SqGj2N4gUPz_",
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'error': 'Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\\n  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\\n  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\\n  libavutil      56. 70.100 / 56. 70.100\\n  libavcodec     58.134.100 / 58.134.100\\n  libavformat    58. 76.100 / 58. 76.100\\n  libavdevice    58. 13.100 / 58. 13.100\\n  libavfilter     7.110.100 /  7.110.100\\n  libswscale      5.  9.100 /  5.  9.100\\n  libswresample   3.  9.100 /  3.  9.100\\n  libpostproc    55.  9.100 / 55.  9.100\\n/content/L1vXCYZAYYM.mp3: No such file or directory\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "614159db",
      "metadata": {
        "id": "614159db"
      },
      "outputs": [],
      "source": [
        "# audio tool\n",
        "import whisper\n",
        "from llama_index.core import Document\n",
        "\n",
        "def get_audio_transcript(file_path: str) -> dict:\n",
        "    model = whisper.load_model(\"base\")\n",
        "    try:\n",
        "        r = model.transcribe(file_path)\n",
        "        return {\"result\": Document(text=r[\"text\"]).text}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_audio_transcript('/content/99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3.mp3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0aWsRHY92I4",
        "outputId": "dacb18e4-88cf-45ce-de38-cf9783d4e6bb"
      },
      "id": "U0aWsRHY92I4",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:06<00:00, 22.6MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'result': ' In a saucepan, combine ripe strawberries, granulated sugar, freshly squeezed lemon juice and cornstarch. Cook the mixture over medium heat, stirring constantly until it thickens to a smooth consistency. Remove from heat and stir in a dash of pure vanilla extract. Allow the strawberry pie feeling to cool before using it as a delicious and fruity filling for your pie crust.'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "b793f34b",
      "metadata": {
        "id": "b793f34b"
      },
      "outputs": [],
      "source": [
        "# python file execution tool\n",
        "import io\n",
        "import contextlib\n",
        "\n",
        "def run_python_file(file_path: str) -> dict:\n",
        "    \"\"\"Safely runs a Python script and returns its final printed numeric output.\"\"\"\n",
        "\n",
        "    try:\n",
        "        python_code = open(file_path).read()\n",
        "\n",
        "        # Create a buffer to capture stdout\n",
        "        buffer = io.StringIO()\n",
        "\n",
        "        # Redirect stdout to the buffer\n",
        "        with contextlib.redirect_stdout(buffer):\n",
        "            exec(python_code)\n",
        "\n",
        "        # Get everything that was printed in the script\n",
        "        output = buffer.getvalue()\n",
        "        last_output = output.split('\\n')[-2]\n",
        "\n",
        "        if 'error' in output.lower():\n",
        "            return {'error': f\"Error running script:\\n{output}\"}\n",
        "\n",
        "        return {'result': f\" Whole output: '{output}'. Final numeric output: {last_output}\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {'error': f\"Execution failed: {str(e)}\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_python_file('/content/f918266a-b3e0-4914-865d-4faa564f1aef.py')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVv4qMGxEVGf",
        "outputId": "1b69be96-4886-422e-ca9c-33120006d8bf"
      },
      "id": "TVv4qMGxEVGf",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'result': \" Whole output: 'Working...\\nPlease wait patiently...\\n0\\n'. Final numeric output: '0' \"}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "97df9b65",
      "metadata": {
        "id": "97df9b65"
      },
      "outputs": [],
      "source": [
        "# Excel tool\n",
        "\n",
        "def get_info_from_excel(file_path: str) -> dict:\n",
        "    \"\"\"Fetch information from an Excel file.\"\"\"\n",
        "\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "        text = df.to_markdown()  # Convert DataFrame to Markdown for better readability\n",
        "        document = Document(text=text)\n",
        "        return {'result': document.text}\n",
        "    except Exception as e:\n",
        "        return {'error': f\"Failed to fetch data from Excel: {str(e)}\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_info_from_excel('/content/7bd855d8-463d-4ed5-93ca-5fe35145f733.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HROHBE2KEjPg",
        "outputId": "757e5299-a93e-4c6b-afbd-c230aba2100f"
      },
      "id": "HROHBE2KEjPg",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'result': '|    | Location    |   Burgers |   Hot Dogs |   Salads |   Fries |   Ice Cream |   Soda |\\n|---:|:------------|----------:|-----------:|---------:|--------:|------------:|-------:|\\n|  0 | Pinebrook   |      1594 |       1999 |     2002 |    2005 |        1977 |   1980 |\\n|  1 | Wharvton    |      1983 |       2008 |     2014 |    2015 |        2017 |   2018 |\\n|  2 | Sagrada     |      2019 |       2022 |     2022 |    2023 |        2021 |   2019 |\\n|  3 | Algrimand   |      1958 |       1971 |     1982 |    1989 |        1998 |   2009 |\\n|  4 | Marztep     |      2015 |       2016 |     2018 |    2019 |        2021 |   2022 |\\n|  5 | San Cecelia |      2011 |       2010 |     2012 |    2013 |        2015 |   2016 |\\n|  6 | Pimento     |      2017 |       1999 |     2001 |    2003 |        1969 |   2967 |\\n|  7 | Tinseles    |      1967 |       1969 |     1982 |    1994 |        2005 |   2006 |\\n|  8 | Rosdale     |      2007 |       2009 |     2021 |    1989 |        2005 |   2011 |'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7ebb6a36",
      "metadata": {
        "id": "7ebb6a36"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "calculator_tool = FunctionTool.from_defaults(\n",
        "    fn=calculate,\n",
        "    name=\"calculator\",\n",
        "    description=\"A calculator that performs basic arithmetic operations.\"\n",
        ")\n",
        "\n",
        "youtube_tool = FunctionTool.from_defaults(\n",
        "    fn=get_youtube_transcript,\n",
        "    name=\"youtube_video_parser\",\n",
        "    description=\"A transcript extractor for youtube videos based on video path.\"\n",
        ")\n",
        "\n",
        "audio_tool = FunctionTool.from_defaults(\n",
        "    fn=get_audio_transcript,\n",
        "    name=\"audio_parser\",\n",
        "    description=\"A simple transcript extractor for audio based on file.\"\n",
        ")\n",
        "\n",
        "run_python_tool = FunctionTool.from_defaults(\n",
        "    fn=run_python_file,\n",
        "    name=\"python_code_executor\",\n",
        "    description=\"Executes a .py file and returns the final printed numeric result.\"\n",
        ")\n",
        "\n",
        "excel_tool = FunctionTool.from_defaults(\n",
        "    fn=get_info_from_excel,\n",
        "    name=\"excel_parser\",\n",
        "    description=\"A simple tool to extract information from an Excel file and trasform it into markdown text.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f579b28b",
      "metadata": {
        "id": "f579b28b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c734eaef-a4ba-4cd0-e0d2-0fc7694ea445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "created_tools = [calculator_tool, youtube_tool, audio_tool, run_python_tool, excel_tool]\n",
        "print(len(created_tools))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = getpass(\"Enter your Tavily API key: \")\n",
        "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")"
      ],
      "metadata": {
        "id": "HUZ8elIpMDVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db4659a8-a447-4d8b-eec8-419a36bc0441"
      },
      "id": "HUZ8elIpMDVO",
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Tavily API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6f09cdbb",
      "metadata": {
        "id": "6f09cdbb"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools.tavily_research.base import TavilyToolSpec\n",
        "from llama_index.tools.wikipedia import WikipediaToolSpec\n",
        "\n",
        "wikipedia_tool_spec = WikipediaToolSpec()\n",
        "\n",
        "# to search for information on the web\n",
        "tavily_tool_spec = TavilyToolSpec(api_key=TAVILY_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd14dc35",
      "metadata": {
        "id": "bd14dc35"
      },
      "source": [
        "To process images I will create another agent that is based on a Vision model as llm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed4d2ac9",
      "metadata": {
        "id": "ed4d2ac9"
      },
      "source": [
        "# Run agent on question to test it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b5abd4f9",
      "metadata": {
        "id": "b5abd4f9"
      },
      "outputs": [],
      "source": [
        "#'''\n",
        "def create_system_prompt_for_main_agent(tools):\n",
        "    \"\"\"Create a more descriptive system prompt that explains the available tools\"\"\"\n",
        "    tool_descriptions = \"\\n\".join([\n",
        "        f\"- {tool._metadata.name}: {tool._metadata.description}\"\n",
        "        for tool in tools\n",
        "    ])\n",
        "\n",
        "    system_prompt = f\"\"\"You're a helpful general AI assistant with the ability to use tools.\n",
        "\n",
        "You have access to the following tools:\n",
        "{tool_descriptions}\n",
        "\n",
        "When a user's request requires using one of these tools:\n",
        "1. First think through what information you need and which tool would be appropriate\n",
        "2. Then provide a clear explanation to the user about your approach\n",
        "3. Finally use the appropriate tool by including the necessary parameters\n",
        "\n",
        "Important: If a question requires calculation, execution of python code or parsing of youtube video, audios or Excel file, ALWAYS use the appropriate\n",
        "tool rather than trying to answer from your knowledge. It doesn't matter if  the answer is straightforward, use tools to ensure accuracy and reliability.\n",
        "\n",
        "Begin.\"\"\"\n",
        "\n",
        "    return system_prompt\n",
        "#''';"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ae50bda4",
      "metadata": {
        "id": "ae50bda4"
      },
      "outputs": [],
      "source": [
        "system_prompt = create_system_prompt_for_main_agent(created_tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "754293f7",
      "metadata": {
        "id": "754293f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46a98c7b-4334-4de2-d4e5-da57b559cd0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You're a helpful general AI assistant with the ability to use tools.\n",
            "\n",
            "You have access to the following tools:\n",
            "- calculator: A calculator that performs basic arithmetic operations.\n",
            "- youtube_video_parser: A transcript extractor for youtube videos based on video path.\n",
            "- audio_parser: A simple transcript extractor for audio based on file.\n",
            "- python_code_executor: Executes a .py file and returns the final printed numeric result.\n",
            "- excel_parser: A simple tool to extract information from an Excel file and trasform it into markdown text.\n",
            "\n",
            "When a user's request requires using one of these tools:\n",
            "1. First think through what information you need and which tool would be appropriate\n",
            "2. Then provide a clear explanation to the user about your approach\n",
            "3. Finally use the appropriate tool by including the necessary parameters\n",
            "\n",
            "Important: If a question requires calculation, execution of python code or parsing of youtube video, audios or Excel file, ALWAYS use the appropriate\n",
            "tool rather than trying to answer from your knowledge. It doesn't matter if  the answer is straightforward, use tools to ensure accuracy and reliability.\n",
            "\n",
            "Begin.\n"
          ]
        }
      ],
      "source": [
        "print(system_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "18b03119",
      "metadata": {
        "id": "18b03119"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent.workflow import ReActAgent\n",
        "\n",
        "multi_agent = ReActAgent(\n",
        "    name='multi_functional_agent',\n",
        "    description=\"A general AI assistant that can use perform calculation, parse files, and execute code.\",\n",
        "    system_prompt=system_prompt,  # must be string\n",
        "    tools=created_tools,\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    can_handoff_to=['wikipedia_agent', 'search_agent']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "543c7159",
      "metadata": {
        "id": "543c7159"
      },
      "outputs": [],
      "source": [
        "def create_system_prompt_for_others(tools):\n",
        "    \"\"\"Create a more descriptive system prompt that explains the available tools\"\"\"\n",
        "    tool_descriptions = \"\\n\".join([\n",
        "        f\"- {tool._metadata.name}: {tool._metadata.description}\"\n",
        "        for tool in created_tools\n",
        "    ])\n",
        "\n",
        "    system_prompt = f\"\"\"You're a helpful general AI assistant with the ability to use tools.\n",
        "\n",
        "You have access to the following tools:\n",
        "{tool_descriptions}\n",
        "\n",
        "When a user's request requires using one of these tools:\n",
        "1. First think through what information you need and which tool would be appropriate\n",
        "2. Then provide a clear explanation to the user about your approach\n",
        "3. Finally use the appropriate tool by including the necessary parameters\n",
        "\n",
        "Important: ALWAYS use the appropriate tools rather than trying to answer from your knowledge.\n",
        "It doesn't matter if  the answer is straightforward, use tools to ensure accuracy and reliability.\n",
        "\n",
        "Begin.\"\"\"\n",
        "\n",
        "    return system_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "7fb53ee5",
      "metadata": {
        "id": "7fb53ee5"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent.workflow import FunctionAgent\n",
        "\n",
        "wiki_agent = ReActAgent(\n",
        "    name='wikipedia_agent',\n",
        "    description=\"A general AI assistant that can search Wikipedia for information.\",\n",
        "    system_prompt=create_system_prompt_for_others(wikipedia_tool_spec.to_tool_list()),\n",
        "    tools=wikipedia_tool_spec.to_tool_list(),\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    can_handoff_to=['multi_functional_agent', 'search_agent']\n",
        ")\n",
        "\n",
        "search_agent = ReActAgent(\n",
        "    name='search_agent',\n",
        "    description=\"A general AI assistant that can search the web for information.\",\n",
        "    system_prompt=create_system_prompt_for_others(tavily_tool_spec.to_tool_list()),\n",
        "    tools=tavily_tool_spec.to_tool_list(),\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    can_handoff_to=['multi_functional_agent', 'wiki_agent']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import AgentWorkflow\n",
        "import requests\n",
        "\n",
        "# Create the workflow\n",
        "workflow = AgentWorkflow(\n",
        "    agents=[multi_agent, wiki_agent, search_agent],\n",
        "    root_agent=\"multi_functional_agent\",\n",
        "    initial_state = {\n",
        "        'file_path': f'/content/{file_name}',\n",
        "    }\n",
        ")\n",
        "\n",
        "if file_name != None:\n",
        "  url = f\"https://agents-course-unit4-scoring.hf.space/files/{task_id}\"\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Save the file\n",
        "  with open(file_name, \"wb\") as f:\n",
        "      f.write(response.content)\n",
        "\n",
        "  print(\"Downloaded:\", file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0HdiRjJr0Dv",
        "outputId": "4085de39-9836-4dea-cab9-9ea5369517c4"
      },
      "id": "D0HdiRjJr0Dv",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: 99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "736df507",
      "metadata": {
        "id": "736df507"
      },
      "outputs": [],
      "source": [
        "extract_prompt = f\"\"\"\n",
        "Answer to the following input question using the agents available to you:\n",
        "\n",
        "{{question}}\n",
        "\n",
        "If one agent does not have the right tools to answer the question, it can hand off to another agent that has the right tools.\n",
        "If the question requires a file, you can use the file name provided in the initial state.\n",
        "\n",
        "Finish your answer with just YOUR FINAL ANSWER (do not ouput <think> or <tool> tags):\n",
        "- YOUR FINAL ANSWER should be a number OR as few words as possible OR a comma separated list of numbers and/or strings.\n",
        "- If you are asked for a number, don't use comma to write your number neither use units such as $ or percent sign unless specified otherwise.\n",
        "- If you are asked for a string, don't use articles, neither abbreviations (e.g. for cities), and write the digits in plain text unless specified otherwise.\n",
        "- If you are asked for a comma separated list, apply the above rules depending of whether the element to be put in the list is a number or a string.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define prompt\n",
        "extract_prompt = extract_prompt.format(question=question_text)"
      ],
      "metadata": {
        "id": "HvuKTAxBsEmY"
      },
      "id": "HvuKTAxBsEmY",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "cf62ba27",
      "metadata": {
        "id": "cf62ba27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef1e125e-a7ac-41d6-8794-c0500fcaa4f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: The current language of the user is: English. I need to use a tool to extract the ingredients from the audio file.\n",
            "Action: audio_parser\n",
            "Action Input: {\"file_path\": \"/content/99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3.mp3\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I have the transcript from the audio file. Now I need to extract the ingredients and format them as requested.\n",
            "Action: python_code_executor\n",
            "Action Input: {'input': \"transcript = ' In a saucepan, combine ripe strawberries, granulated sugar, freshly squeezed lemon juice and cornstarch. Cook the mixture over medium heat, stirring constantly until it thickens to a smooth consistency. Remove from heat and stir in a dash of pure vanilla extract. Allow the strawberry pie feeling to cool before using it as a delicious and fruity filling for your pie crust.'\\n\\ningredients = []\\nif 'ripe strawberries' in transcript:\\n    ingredients.append('ripe strawberries')\\nif 'granulated sugar' in transcript:\\n    ingredients.append('granulated sugar')\\nif 'freshly squeezed lemon juice' in transcript:\\n    ingredients.append('lemon juice')\\nif 'cornstarch' in transcript:\\n    ingredients.append('cornstarch')\\nif 'pure vanilla extract' in transcript:\\n    ingredients.append('vanilla extract')\\n\\ningredients.sort()\\n\\nprint(', '.join(ingredients))\"}Thought: I made a mistake in the previous step. I need to correct the code.\n",
            "Action: python_code_executor\n",
            "Action Input: {'input': \"transcript = ' In a saucepan, combine ripe strawberries, granulated sugar, freshly squeezed lemon juice and cornstarch. Cook the mixture over medium heat, stirring constantly until it thickens to a smooth consistency. Remove from heat and stir in a dash of pure vanilla extract. Allow the strawberry pie feeling to cool before using it as a delicious and fruity filling for your pie crust.'\\n\\ningredients = []\\nif 'ripe strawberries' in transcript:\\n    ingredients.append('ripe strawberries')\\nif 'granulated sugar' in transcript:\\n    ingredients.append('granulated sugar')\\nif 'freshly squeezed lemon juice' in transcript:\\n    ingredients.append('lemon juice')\\nif 'cornstarch' in transcript:\\n    ingredients.append('cornstarch')\\nif 'pure vanilla extract' in transcript:\\n    ingredients.append('vanilla extract')\\n\\ningredients.sort()\\n\\nprint(', '.join(ingredients))\"}Thought: I made another mistake in the previous step. I need to correct the code.\n",
            "Action: python_code_executor\n",
            "Action Input: {'input': \"transcript = ' In a saucepan, combine ripe strawberries, granulated sugar, freshly squeezed lemon juice and cornstarch. Cook the mixture over medium heat, stirring constantly until it thickens to a smooth consistency. Remove from heat and stir in a dash of pure vanilla extract. Allow the strawberry pie feeling to cool before using it as a delicious and fruity filling for your pie crust.'\\n\\ningredients = []\\nif 'ripe strawberries' in transcript:\\n    ingredients.append('ripe strawberries')\\nif 'granulated sugar' in transcript:\\n    ingredients.append('granulated sugar')\\nif 'lemon juice' in transcript:\\n    ingredients.append('lemon juice')\\nif 'cornstarch' in transcript:\\n    ingredients.append('cornstarch')\\nif 'vanilla extract' in transcript:\\n    ingredients.append('vanilla extract')\\n\\ningredients.sort()\\n\\nprint(', '.join(ingredients))\"}Thought: I am still making mistakes. I will try a different approach to extract the ingredients.\n",
            "Action: python_code_executor\n",
            "Action Input: {'input': \"transcript = ' In a saucepan, combine ripe strawberries, granulated sugar, freshly squeezed lemon juice and cornstarch. Cook the mixture over medium heat, stirring constantly until it thickens to a smooth consistency. Remove from heat and stir in a dash of pure vanilla extract. Allow the strawberry pie feeling to cool before using it as a delicious and fruity filling for your pie crust.'\\n\\ningredients = []\\nif 'strawberries' in transcript:\\n    ingredients.append('ripe strawberries')\\nif 'sugar' in transcript:\\n    ingredients.append('granulated sugar')\\nif 'lemon juice' in transcript:\\n    ingredients.append('lemon juice')\\nif 'cornstarch' in transcript:\\n    ingredients.append('cornstarch')\\nif 'vanilla extract' in transcript:\\n    ingredients.append('vanilla extract')\\n\\ningredients.sort()\\n\\nprint(', '.join(ingredients))\"}Thought: I am still struggling with the code. I will try a simpler approach.\n",
            "Action: python_code_executor\n",
            "Action Input: {'input': \"transcript = ' In a saucepan, combine ripe strawberries, granulated sugar, freshly squeezed lemon juice and cornstarch. Cook the mixture over medium heat, stirring constantly until it thickens to a smooth consistency. Remove from heat and stir in a dash of pure vanilla extract. Allow the strawberry pie feeling to cool before using it as a delicious and fruity filling for your pie crust.'\\n\\ningredients = []\\nif 'strawberries' in transcript:\\n    ingredients.append('ripe strawberries')\\nif 'sugar' in transcript:\\n    ingredients.append('granulated sugar')\\nif 'lemon juice' in transcript:\\n    ingredients.append('lemon juice')\\nif 'cornstarch' in transcript:\\n    ingredients.append('cornstarch')\\nif 'vanilla' in transcript:\\n    ingredients.append('vanilla extract')\\n\\ningredients.sort()\\n\\nprint(', '.join(ingredients))\"}Thought: I am still having trouble with the python code. I will try a different approach. I will manually extract the ingredients.\n",
            "Action: calculator\n",
            "Action Input: {}\n",
            "Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
            "Answer: cornstarch, granulated sugar, lemon juice, ripe strawberries, vanilla extract\n"
          ]
        }
      ],
      "source": [
        "#import nest_asyncio\n",
        "from llama_index.core.memory import Memory\n",
        "#nest_asyncio.apply()\n",
        "\n",
        "# to keep conversation history\n",
        "memory = Memory.from_defaults(token_limit=40000)\n",
        "\n",
        "# run agent with thinking process printed\n",
        "async def main():\n",
        "  handler = workflow.run(extract_prompt, memory=memory)\n",
        "\n",
        "  # così vedo come ragiona il LLM\n",
        "  async for event in handler.stream_events():\n",
        "      if hasattr(event, \"delta\"):\n",
        "          print(event.delta, end=\"\", flush=True)\n",
        "\n",
        "  result = await handler\n",
        "  return result\n",
        "\n",
        "result = await main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "42887665",
      "metadata": {
        "id": "42887665",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ca87312-1251-42c2-9258-d2208f83d3ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cornstarch, granulated sugar, lemon juice, ripe strawberries, vanilla extract\n"
          ]
        }
      ],
      "source": [
        "print(result.response.blocks[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc0a25bd",
      "metadata": {
        "id": "cc0a25bd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}